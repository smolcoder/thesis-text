\chapter{Задача автоматического тегирования}
\label{chapter1}

В данной главе формулируется задача автоматического тегирования, описываются основные этапы ее решения, 
начиная с извлечения характеристических признаков и заканчивая обзором алгоритмов машинного обучения.

\section{Постановка общей задачи}
Здесь и далее \emph{треком} будем называть цифровое представление акустических волн некоторой музыкальной композиции (например, форматы \emph{mp3}, \emph{midi}, \emph{wav}).

Пусть имеется множество песен $S = \{s_1, s_2, \ldots, s_D \}$, а также словарь $ \mcV $, состоящий из $V = |\mcV| $ уникальных слов. 
Каждое ``слово'' $w_i \in \mcV$ представляет из себя семантический тег, например ``рок'', ``грусть'', ``счастье'', ``фортепиано''.

Задачу автоматического тегирования можно условно представить~\cite{msordo_thesis} в виде двух смежных задач: 
\begin{itemize}
 \item нахождение тегов, наиболее характерных (семантически значимых) для заданного трека (\emph{annotation problem});
 \item поиск релевантных песен по заданному тегу (\emph{retrieval problem}).
\end{itemize}

Первая задача может рассматриваться как мультиклассовая классификация~\cite{multilabelclassification, multilabel_1}, где метками классов является множество $\mcV$.
Во второй задаче проверяется, на сколько хорошо алгоритм автоматического тегирования способен выбирать по заданному тегу наиболее подходящие треки~\cite{msordo_thesis}.

Опишем задачи формально.
Рассмотрим вектор $\mb{y} = (y_1, \ldots, y_V)$, где $y_i > 0$ означает, что $w_i$ соотносится с треком, $y_i = 0$ в противном случае. То есть $\mb{y} \in \R^{V}_{+}$.
Назовем его \emph{семантическим вектором}.

Задача сопоставления тегов непротегированному треку $s_q$ заключается в нахождении такого семантического вектора $\mb{y}$, 
что множество тегов $\mcW = \{w_i \mid w_i \in \mcV, y_i > 0\}$ наиболее точно характеризует трек $s_q$.

Задача поиска релевантных песен по тегу $w_q$ заключается в формировании упорядоченного множества $R = (s_{i_1}, \ldots, s_{i_D})$ такого, что:
\begin{itemize}
 \item $i_j \in \{1, \ldots, D \}$;
 \item обозначим семантический вектор $s_{i_a}$ за $y^a$, $s_{i_b}$ \ld за $y^b$, тогда $a > b \Rightarrow y^a_q >= y^b_q $.
\end{itemize}
В данной задаче величина $y^m_i$ семантического вектора интерпретируется как степень принадлежности тега $w_i$ треку $s_{i_m}$. 
 
\section{Просесс автоматического тегирования}

Условно процесс автоматического тегирования можно разделить на следующие части:
\begin{itemize}
 \item извлечение харастеристических признаков (feature extraction);
 \item выбор характеристических признаков (feature selection);
 \item понижение размерности (dimension reduction) пространства характеристических признаков;
 \item применение алгоритма машинного обучения для тегирования.
\end{itemize}

Далее мы рассмотрим каждую из этих частей подробно. Также будут рассмотрены метрики расстояний и способы оценки качества тегирования.

\subsection{Извлечение характеристических признаков}

В ~\cite{orio} установлено, что наиболее важными \emph{характеристическими признаками} (аспектами), 
которыми человек характеризует музыку, являются тембр, музыкальный инструмент, акустические аспекты, ритм, мелодия и гармония.
Эти признаки можно извлекать непосредственно из акустических волн, представленных в цифровом аудиофайле. Цель данного извлечения \ld компактное и удобное
представление аудио, которое отразит вышеупомянутые признаки для дальнейшей работы с ними.

Приведем краткое описание процесса извлечения, более подробно описанного в ~\cite{msordo_thesis}.
Характеристические признаки варьируются от низкоуровневых (уровень сигнала) до высокоуровневых (уровень человеческого восприятия).
В данной работе используются различные низкоуровневые признаки (уровень сигнала), например, такие, как центроиды, спады, эксцессы, мел-кепстральные коэффициенты.
Также имеются ритм-признаки, например (bpm{bpm\ld beats per minute}), признаки тональности (музыкальные ключи, аккорды). К высокоуровневым признакам относятся, 
например, настроение, ``танцевальность''.

Для извлечения характеристических признаков в данной работе используется инструмент \emph{Essentia}~\cite{essentia}, написанный на С++. 
В ~\cite{essentia} приводится более детальное описание всех характеристических признаков, которые извлекаются данным инструментом. 
В процессе извлечения каждый аудиофайл разбивается на короткие (около 46-50 мс) \emph{фреймы}, где каждый фрейм пересекается с предыдущим 
на половину для избежания нежелательных краевых эффектов.
Затем из каждого такого промежутка извлекаются характеристические признаки. В результате для каждого фрейма имеется набор признаков.
В данном исследовании использовалось 188 различных признаков.

\subsection{Выбор характеристических признаков}

Рассмотрим на примере, сколько информации необходимо хранить после извлечения признаков.
Представим себе трек длиной 5 минут. Для конкретного характеристического признака, извлеченного из фрейма длиной 46 мс с перекрытием длиной
23 мс приведет к вектору размерностью $\frac{5 * 60 * 1000 - 1}{23} \simeq 13043$, если значение признака состоит из одного числа. На практике
же некоторые признаки сами по себе являются векторами. Если значения всех признаков (в нашем случае их 188) последовательно друг за другом 
записать в один вектор, то он будет иметь размерность 523.
Как следствие, у данного подхода к представлению треков есть существенные недостатки:
\begin{itemize}
 \item треки разной длины будут состоять из разного количества векторов;
 \item необходимо хранить слишком много информации.
\end{itemize}
Поэтому вместо значений характеристических признаков на всех фреймах хранят среднее, дисперсию по всем фреймам, а также их первые производные.
Таким образом на каждый трек приходится один вектор из 523 значений.

\subsection{Сужение пространства характеристических признаков}

Чтобы еще уменьшить объем хранимых данных без существенной потери информации о характеристических признаках, применяется \emph{метод главных компонент} (PCA, principal component analysis).
Это распространенная техника для уменьшения размерности данных (особенно тех, графическое представление которых физически невозможно) с минимальной потери информации \cite{msordo_thesis, pca}. 

Опишем основную идею подхода. Пусть имеется вектор размерностью $d$ ($d$-вектор), представляющий характеристические признаки трека, тогда PCA заключается в проецировании исходного пространства
на такое новое подпространство, что разброс (дисперсия) данных в ортогональных проекциях будет максимальным. Пусть $\Phi$ \ld отображение из исходного $d$-мерного пространства $X$ характеристических
признаков в новое $f$-мерное пространство (обычно $f < d$). Тогда новые вектора признаков $y_i \in \R^f$ будут вычисляться как:
$$ y_i = \phi_i^T(X - \mu) $$,
где $\mu$ \ld среднее векторов всех треков в исходном пространстве. $\Phi$ представляет из себя матрицу, столбцами которой являются собственные вектора $\phi_i$, получаемые из уравнений
$$\Sigma_X \phi_i = \lambda_i\phi_i $$,
где $\Sigma_X$ \ld матрица ковариации, $\lambda_i$ \ld собственное число, соответствующее собственному вектору $\phi_i$. Причем собственные вектора с б\'{о}льшими собственными числами покрывают
больше дисперсии, поэтому вектора с меньшими собственными числами можно не учитывать, сократив при этом размерность итогового пространства с малой потерей информации. Для наглядности 
перенумеруем собственные вектора так, чтобы
$$|\lambda_1| \lte |\lambda_2| \gte \ldots |\lambda_d|$$.
Тогда в первых $n \lte d$ векторах будет содержаться 
$$\frac{\sum_{i=1}^{n} |\lambda_i|}{\sum_{i=1}^{d} |\lambda_i|} \times 100\%$$ 
информации. Таким образом, чем больше характеристические признаки коррелируют друг с другом, тем меньше собственных векторов будут иметь большие собственные значения, и, как следствие, размерность нового подпространства
будет меньше.

\subsection{Метрики расстояний}
\subsection{Обзор алгоритмов ближайших соседей}
\subsubsection{Взвешенный k-NN}
\subsubsection{CBDC}
\subsection{Оценка эффективности}
\section{Выводы}





