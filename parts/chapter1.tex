\chapter{Задача автоматического тегирования}
\label{chapter1}

В данной главе формулируется задача автоматического тегирования, описываются основные этапы ее решения, 
начиная с извлечения характеристических признаков и заканчивая обзором алгоритмов машинного обучения.

\section{Постановка общей задачи}
Здесь и далее \emph{треком} будем называть цифровое представление акустических волн некоторой музыкальной композиции (например, форматы \emph{mp3}, \emph{midi}, \emph{wav}).

Пусть имеется множество песен $S = \{s_1, s_2, \ldots, s_D \}$, а также словарь $ \mcV $, состоящий из $V = |\mcV| $ уникальных слов. 
Каждое ``слово'' $w_i \in \mcV$ представляет из себя семантический тег, например ``рок'', ``грусть'', ``счастье'', ``фортепиано''.

Задачу автоматического тегирования можно условно представить~\cite{turnbull, msordo_thesis} в виде двух смежных задач: 
\begin{itemize}
 \item нахождение тегов, наиболее характерных (семантически значимых) для заданного трека (\emph{annotation problem});
 \item поиск релевантных песен по заданному тегу (\emph{retrieval problem}).
\end{itemize}

Первая задача может рассматриваться как мультиклассовая классификация~\cite{multilabelclassification, multilabel_1}, где метками классов является множество $\mcV$.
Во второй задаче проверяется, на сколько хорошо алгоритм автоматического тегирования способен выбирать по заданному тегу наиболее подходящие треки~\cite{turnbull}.

Опишем задачи формально.
Рассмотрим вектор $\mb{y} = (y_1, \ldots, y_V)$, где $y_i > 0$ означает, что $w_i$ соотносится с треком, $y_i = 0$ в противном случае. То есть $\mb{y} \in \R^{V}_{+}$.
Назовем его \emph{семантическим вектором}.

Задача сопоставления тегов непротегированному треку $s_q$ заключается в нахождении такого семантического вектора $\mb{y}$, 
что множество тегов $\mcW = \{w_i \mid w_i \in \mcV, y_i > 0\}$ наиболее точно характеризует трек $s_q$.

Задача поиска релевантных песен по тегу $w_q$ заключается в формировании упорядоченного множества $R = (s_{i_1}, \ldots, s_{i_D})$ такого, что:
\begin{itemize}
 \item $i_j \in \{1, \ldots, D \}$;
 \item обозначим семантический вектор $s_{i_a}$ за $y^a$, $s_{i_b}$ \ld за $y^b$, тогда $a > b \Rightarrow y^a_q >= y^b_q $.
\end{itemize}
В данной задаче величина $y^m_i$ семантического вектора интерпретируется как степень принадлежности тега $w_i$ треку $s_{i_m}$. 
 
\section{Просесс автоматического тегирования}

Условно процесс автоматического тегирования можно разделить на следующие части:
\begin{itemize}
 \item извлечение характеристических признаков (feature extraction);
 \item выбор характеристических признаков (feature selection);
 \item понижение размерности (dimension reduction) пространства характеристических признаков;
 \item применение алгоритма машинного обучения для тегирования.
\end{itemize}

Далее мы рассмотрим каждую из этих частей подробно. Также будут рассмотрены метрики расстояний и способы оценки качества тегирования.

\subsection{Извлечение характеристических признаков}

В ~\cite{orio} установлено, что наиболее важными \emph{характеристическими признаками} (аспектами), 
которыми человек характеризует музыку, являются тембр, музыкальный инструмент, акустические аспекты, ритм, мелодия и гармония.
Эти признаки можно извлекать непосредственно из акустических волн, представленных в цифровом аудиофайле. Цель данного извлечения \ld компактное и удобное
представление аудио, которое отразит вышеупомянутые признаки для дальнейшей работы с ними.

Приведем краткое описание процесса извлечения, более подробно описанного в ~\cite{msordo_thesis}.
В данной работе используются различные низкоуровневые признаки (уровень сигнала), например, такие, как центроиды, спады, эксцессы, мел-кепстральные коэффициенты.
Также имеются ритм-признаки (например, bpm (beats per minute)), признаки тональности (музыкальные ключи, аккорды). К высокоуровневым (уровень человеческого восприятия) признакам 
относятся, например, настроение, ``танцевальность''.

Для извлечения характеристических признаков в данной работе используются инструменты \emph{Essentia} и \emph{Gaia} ~\cite{essentia}, написанные на С++. 
В ~\cite{essentia} приводится более детальное описание всех характеристических признаков, которые извлекаются данным инструментом. 
В процессе извлечения каждый аудио-файл разбивается на короткие (около 46-50 мс) \emph{фреймы}, где каждый фрейм пересекается с предыдущим 
на половину для избежания нежелательных краевых эффектов. Для извлечения высокоуровневых признаков в Gaia натренированы классификаторы.
Затем из каждого такого промежутка извлекаются характеристические признаки. В результате для каждого фрейма имеется набор признаков.
В данном исследовании использовалось 188 различных признаков.

\subsection{Выбор характеристических признаков}

Рассмотрим на примере, сколько информации необходимо хранить после извлечения признаков.
Представим себе трек длиной 5 минут. Для информации о конкретном характеристическом признаке, извлеченного из фрейма длиной 46 мс с перекрытием длиной
23 мс, потребуется вектор размерностью $\frac{5 * 60 * 1000 - 1}{23} \simeq 13043$, если значение признака состоит из одного числа. На практике
же некоторые признаки сами по себе являются векторами. Если значения всех признаков (в нашем случае их 188) последовательно друг за другом 
записать в один вектор, то он будет иметь размерность 523.
Как следствие, у данного подхода к представлению треков есть существенные недостатки:
\begin{itemize}
 \item треки разной длины будут состоять из разного количества векторов;
 \item необходимо хранить слишком много информации.
\end{itemize}
Поэтому вместо значений характеристических признаков на всех фреймах хранят~\cite{msordo_thesis, essentia} среднее, дисперсию по всем фреймам, а также их первые производные.
Таким образом на каждый трек приходится один вектор из 523 значений.

\subsection{Сужение пространства характеристических признаков}

Чтобы еще уменьшить объем хранимых данных без существенной потери информации о характеристических признаках, в данной работе применяется \emph{метод главных компонент} (PCA, principal component analysis).
Это распространенная техника для уменьшения размерности данных (особенно тех, графическое представление которых физически невозможно) с минимальной потери информации \cite{msordo_thesis, pca}. 

Опишем основную идею подхода. Пусть имеется вектор размерностью $d$ ($d$-вектор), представляющий характеристические признаки трека, тогда PCA заключается в проецировании исходного пространства
на такое новое подпространство, что дисперсия данных в ортогональных проекциях будет максимальной. Пусть $\Phi$ \ld отображение из исходного $d$-мерного пространства $X$ характеристических
признаков в новое $f$-мерное пространство (обычно $f < d$). Тогда новые вектора признаков $y_i \in \R^f$ будут вычисляться как:
$$ y_i = \phi_i^T(X - \mu) $$,
где $\mu$ \ld среднее векторов всех треков в исходном пространстве. $\Phi$ представляет из себя матрицу, столбцами которой являются собственные вектора $\phi_i$, получаемые из уравнений
$$\Sigma_X \phi_i = \lambda_i\phi_i $$,
где $\Sigma_X$ \ld матрица ковариации, $\lambda_i$ \ld собственное число, соответствующее собственному вектору $\phi_i$. Причем собственные вектора с б\'{о}льшими собственными числами покрывают
больше дисперсии, поэтому вектора с меньшими собственными числами можно не учитывать, сократив при этом размерность итогового пространства с малой потерей информации. Для наглядности 
перенумеруем собственные вектора так, чтобы
$$|\lambda_1| \gte |\lambda_2| \gte \ldots \gte |\lambda_d|$$.
Тогда в первых $n \lte d$ векторах будет содержаться 
$$\frac{\sum_{i=1}^{n} |\lambda_i|}{\sum_{i=1}^{d} |\lambda_i|} \times 100\%$$ 
информации. Таким образом, чем больше характеристические признаки коррелируют друг с другом, тем меньше собственных векторов будут иметь большие собственные значения, и, как следствие, размерность нового подпространства
будет меньше.

\subsection{Метрики расстояний}

Результат тегирования алгоритмами ближайших соседей зависит от выбранной метрики, по которой оценивается расстояние между точками, соответствующим трекам.
В данной работе рассматриваются две метрики: евклидова и косинусная. Именно на этих метриках алгоритмы тегирования в исследовании \cite{msordo_thesis} показывали наилучшие результаты.

\emph{Евклидово} расстояние между векторами $\vec{a}$ и $\vec{b}$ определяется следующим образом:
$$ dist(\vec{a}, \vec{b}) = \sqrt{\sum_{i=1}^{N} (a_i - b_i)^2} $$.

\emph{Косинусное} расстояние между векторами $\vec{a}$ и $\vec{b}$ определяется следующим образом:
$$ dist(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\| \vec{a} \| \cdot \| \vec{b} \|} = \frac{\sum_{i=1}^{N} a_i \cdot b_i}{\sqrt{\sum_{i=1}^{N} a_i^2} \cdot \sqrt{\sum_{i=1}^{N} b_i^2}} $$.


\subsection{Обзор алгоритмов ближайших соседей}

Для решения поставленных выше задач сопоставления тегов треку и поиска релевантных треков по тегу будут рассмотрены два алгоритма, основанных на поиске ближайших соседей: 
$k$-NN (\emph{k}-Nearest Neighbors) и CBDC (Class-based Distance Classification). Основная идея первого алгоритма заключается в ассоциации с конкретным треком некоторого подмножества тегов, 
которые имеются у ближайших соседей данного трека. Иногда, в зависимости от того, как точки (треки) расположены в пространстве, каков размер словаря тегов, сколько треков отмечено
каждым тегом, данный алгоритм может не покрывать всех тегов. Это означает, что очень редкие теги не будут ассоциироваться с новыми треками. Для решения этой проблемы 
в ~\cite{msordo_thesis} был предложен второй алгоритм, который основан не на поиске треков, ``похожих по звучанию'' с данным, а на похожести трека непосредственно на теги.

\subsubsection{\emph{k}-Nearest Neighbors}

Основная идея алгоритма $k$-NN применительно к задаче мультиклассовой классификации заключается в следующем. Пусть у нас имеется трек-запрос $s_q$ \ld некоторая точка в $d$-мерном пространстве.
Также имеются треки некоторой размеченной обучающей выборки $ D = \{s_1, s_2, \ldots, s_M \}$ \ld набор точек в том же пространстве, и зафиксирована одна из метрик расстояния. 
Тогда находятся ближайшие к $s_q$ $k$ точек из $D$, теги соседей объединяются в множество $V_q$, затем по определенному правилу выбирается их подмножество, тегами которого будет ассоциирован трек $s_q$.

В данной работе рассматриваются несколько правил выбора результирующих тегов из $V_q$ для $s_q$. 

Пусть для каждого тега нам известна также его частота появления среди ближайших соседей, и теги
в $V_q$ упорядочены по невозрастанию этой величины. Первый способ состоит в том, чтобы ассоциировать с треком первые $n$ тегов из $V_q$.

Второй способ основан на \emph{ограничивающем пороге} $t \in [0, 1]$ таком, что ассоциировать с треком будем только те теги, частота которых не ниже данного порога.
Пусть, например, используется алгоритм $k$-NN, где $k = 10, t = 0.3$, тег $w_1$ встречается 4 раза, а тег $w_2$ \ld 1 раз. 
Тогда частоты данных тегов равны $\frac{4}{10} = 0.4, \frac{1}{10} = 0.1$ соответственно. В этом случае тег $w_2$ выбран не будет, так как $0.1 < 0.3$.
Как отмечается в ~\cite{msordo_thesis}, данный способ выбора тегов хорошо применим к задаче сопоставления тегов треку.

Для задачи поиска треков по тегу лучше подходит третий способ, основанный на \emph{весовой функции}. При данном подходе в качестве $k$ берется количество треков в обучающей выборке.
Весовая функция для тега $t$, ассоциированного с $n$-ым ближайшим соседом выглядит следующим образом:
$$\begin{equation}
weight(t, n) = 
 \begin{cases}
   1, & n \lte k \\
   \frac{1}{n^2}, & n > k
 \end{cases}
\end{equation}$$.

При данном подходе каждый трек будет ассоциирован абсолютно всеми тегами, что очень важно для поисковой задачи. Однако теги ближайших соседей будут иметь наибольший вес.

\subsubsection{Class-based Distance Classification}

Как уже отмечалось выше, данный метод основан на похожести трека на тег. Идея метода заключается в том, что для каждого тега $w_i$ выбираются все треки из обучающей выборки, 
отмеченные этим тегом, затем вычисляется \emph{центр масс (центроид)} точек этих треков (от этого и название class-based). Таким образом получаются $|\mcV|$ точек в том же 
пространстве, что и точки треков. Когда все центроиды посчитаны, алгоритм ассоциирует с треком теги, соответствующие ближайшим $p$ центроидам.
Как отмечается в \cite{msordo_thesis}, данный алгоритм покрывает весь словарь тегов вне зависимости от их частоты появления в обучающей выборке. Также отмечается преимущество во времени работы
данного алгоритма перед $k$-NN. Действительно, время поиска ближайших соседей в последнем будет зависеть от размера обучающей выборки, а в CBDC \ld от размера словаря. 

\subsection{Оценка эффективности}

Для оценки качества тегирования используются различные~\cite{msordo_thesis, prec_recall, turnbull} подходы. 
Проверка алгоритмов тегирования осуществлялась \emph{кросс-валидацией}, при которой $\frac{1}{10}$ часть обучающей выборки бралась для тестирования, 
а $\frac{9}{10}$ \ld для тренировки алгоритма. Далее будут описаны способы оценки качества, которые используются в данном исследовании.

\subsubsection{Задача сопоставления тегов треку}

Как уже отмечалось ранее, задача сопоставления тегов треку может рассматриваться как мультиклассовая классификация, 
основанная на тегах ближайших соседей\footnote{при рассмотрении алгоритмов на основе ближайших соседей}.

Определим некоторые понятия, связанные с результатом определения принадлежности трека к определенному классу (тегу).

\emph{Истинно-положительный} ответ (true-positive, TP) \ld положительный ответ классификатора при истинной принадлежности трека к классу.

\emph{Истинно-отрицательный} ответ (true-negative, TN) \ld отрицательный ответ классификатора при том, что трек не принадлежит классу.

\emph{Ложно-положительный} ответ (false-positive, TP) \ld положительный ответ классификатора при том, что трек не принадлежит классу.

\emph{Ложно-отрицательный} ответ (false-negative, TN) \ld отрицательный ответ классификатора при истинной принадлежности трека к классу. 

Для наглядности определения сведены в таблицу \ref{tab:contingency}.

\begin{center}
\begin{table}[ht]
\centering
\captionsetup{justification=centering}
\caption{Сводная таблица соотношений результатов классификатора и истинных значений.}
\label{tab:contingency}
\begin{tabular}{cc|c|c|}
\cline{3-4}
& & \multicolumn{2}{|c|}{Исходные значения}\\
\cline{3-4}
& & True & False \\
\hline
\multicolumn{1}{ |c| }{\multirow{2}{*}{Результат классификации}}& 
  \multicolumn{1}{ |c| }{Positive} & True Positive \cellcolor{green} & 
  False Positive\cellcolor{red}\\
\cline{2-4}
\multicolumn{1}{ |c| }{} & \multicolumn{1}{ |c| }{Negative} & 
  False Negative\cellcolor{red} & True Negative \cellcolor{green}\\
\hline
\end{tabular}
\end{table}
\end{center}

\emph{Точность} показывает отношение верно угаданных классов к общему количеству предсказанных классов:
$$P = \frac{TP}{TP + FP}$$.

\emph{Полнота} показывает отношение верно угаданных классов к истинному количеству классов:
$$R = \frac{TP}{TP + FN}$$.

$F$-\emph{мера} \ld взвешенное гармоническое среднее точности и полноты:
$$R = \frac{1}{\alpha\frac{1}{P} + (1 - \alpha)\frac{1}{R}} = \frac{(\beta^2 + 1)PR}{\beta^2P + R}$$, 
$\alpha \in [0, 1], \beta^2 = \frac{1-\alpha}{\alpha}$. В данной работе полноте и точности дается одинаковый вес, то есть используется $F_1$-мера, где $\beta = 1$.
$F$-мера используется для того, чтобы объединить полноту и точность в одной величине. Обычное среднее арифметическое не подходит, так как можно присваивать треку 
все классы во всех случаях, увеличивая тем самым полноту и сводя к минимуму точность, добиваясь тем самым высокого значения обычного среднего. $F$-мера же в этом случае
будет приближаться к наименьшей из двух величин.

Каждый из этих показателей может быть рассчитан \emph{относительно трека (per-song)} и \emph{относительно тега (per-word)}~\cite{msordo_thesis, turnbull}.
Первый способ заключается в усреднении вышеупомянутых функций по некоторому набору \emph{треков}. В данном случае результат будет отражать, на сколько хорошо алгоритм тегирует новые треки.
Но этот способ не учитывает покрытие словаря тегов алгоритмом, то есть сопоставляются ли редкие теги новым трекам. Второй способ не имеет этого недостатка, так как там усреднение
производится по \emph{тегам}. Полнота и точность относительно тега вычисляется следующим образом:
$$ Precision = \frac{|W_C|}{|W_A|}, Recall = \frac{|W_C|}{|W_H|} $$,
где $|W_H|$ \ld число треков из обучающей выборки, которые отмечены тегом $w$, $|W_A|$ \ld число треков, автоматически отмеченных тегом $w$ и $|W_C| = |W_C \cap W_H|$.

\subsubsection{Задача поиска релевантных треков по тегу}

В задаче поиска релевантных песен мы пытаемся сымитировать работу поисковой системы, где запросом является тег, а результатом \ld ранжированный по релевантности список треков.
Описанные ранее способы оценки качества пригодны для неупорядоченного множества треков, поэтому необходимы новые подходы.

\emph{Усредненная по запросам средняя точность}. Средняя точность (average precision) для запроса $q_j$ равна 
$$ AveP_j = \frac{1}{M_j} \sum_{n=1}^{M_j} precision(R_{jn})$$,
где $M_j$ \ld истинное количество релевантных треков для тега $q_j$, $R_{jn}$ \ld множество возвращенных треков, начиная с первой позиции в выдаче и заканчивая позицией 
релевантного трека $s_n$. Тогда усредненная средняя точность (Mean Average Precision, MAP) является арифметическим средним величины $AveP_j$ по всем тегам:
$$ MAP = \frac{1}{|Q|} \sum_{j=1}^{|Q|} AveP_j $$, где $Q = \{q_1, q_2, \ldots, q_j\}$ \ld множество запросов. 
Также MAP может быть расценена как площадь под Precision-Recall кривой~\cite{prec_recall, turnbull}.

\emph{Площадь под ROC кривой}. ROC (англ, receiver operating characteristic, операционная характеристика приёмника) кривая \ld график отношения истинно-положительных ответов классификатора
к общему числу положительных ответов. Точки графика строятся по мере прохода вдоль возращенного списка релевантных треков. Площадь под ROC кривой отражает качество выбранного алгоритма применительно
к данной задаче, чем выше величина, тем лучше классификатор. Если значение площади близко к $0.5$, то алгоритм равносилен случайному выбору классов.

\section{Выводы}

В данной главе была сформулирована общая задача тегирования и описан процесс автоматического тегирования, заключающийся в извлечении характеристических признаков, способе их 
оптимального представления и выборе алгоритма машинного обучения для непосредственно тегирования. Также были рассмотрены используемые в данной работе метрики расстояний и 
описаны способы оценки качества тегирования в зависимости от задачи.





